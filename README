CUV Documentation

0.9.0

Summary

CUV is a C++ template and Python library which makes it easy to use NVIDIA(tm)
CUDA.

Features

Supported Platforms:

  • This library was only tested on Ubuntu Karmic. It uses only standard
    components and should run without major modification on any current linux
    system.

Structure:

  • Like for example Matlab, CUV assumes that everything is a matrix or a
    vector.
  • Vectors/Matrices can have an arbitrary type and can be on the host
    (CPU-memory) or device (GPU-memory)
  • Matrices can be column-major or row-major
  • The library defines many functions which may or may not apply to all
    possible combinations. Variations are easy to add.
  • Conversion routines are provided for most cases
  • For convenience, we also wrap some of the functionality provided by Alex
    Krizhevsky on his website (http://www.cs.utoronto.ca/~kriz/) with
    permission. Thanks Alex for providing your code!

Python Integration

  • CUV plays well with python and numpy. That is, once you wrote your fast GPU
    functions in CUDA/C++, you can export them using Boost.Python. You can use
    Numpy for pre-processing and fancy stuff you have not yet implemented, then
    push the Numpy-matrix to the GPU, run your operations there, pull again to
    CPU and visualize using matplotlib. Great.

Implemented Functionality

  • Simple Linear Algebra for dense vectors and matrices (BLAS level 1,2,3)
  • Helpful functors and abstractions
  • Sparse matrices in DIA format and matrix-multiplication for these matrices
  • I/O functions using boost.serialization
  • Fast Random Number Generator
  • Up to now, CUV was used to build dense and sparse Neural Networks and
    Restricted Boltzmann Machines (RBM), convolutional or locally connected.

Documentation

  • Tutorials are available on 
    http://www.ais.uni-bonn.de/~schulz/tag/cuv
  • The documentation can be generated from the code or accessed on the
    internet: http://www.ais.uni-bonn.de/deep_learning/doc/html/index.html

Contact

  • We are eager to help you get going with CUV and improve it continuously!
    http://www.ais.uni-bonn.de/deep_learning/index.html

Installation

Requirements

For C++ libs, you will need:

  • libboost-dev >= 1.37
  • libblas-dev
  • libtemplate-perl -- (we might get rid of this dependency soon)
  • NVIDIA CUDA (tm), including SDK. We support versions 2.X and 3.0.
  • thrust library (from http://code.google.com/p/thrust/)
  • doxygen (if you want to build the documentation yourself)

For Python Integration, you additionally have to install

  • pyublas -- from http://mathema.tician.de/software/pyublas
  • python-dev

Obtaining CUV

You have two choices:

  • Download the tar-file from our website (http://www.ais.uni-bonn.de)
  • Checkout our git repository

       $ git clone git://github.com/deeplearningais/CUV.git

Installation Procedure

Building a debug version:

 $ tar xzvf cuv-version-source.tar.gz
 $ cd cuv-version-source
 $ mkdir -p build/debug
 $ cd build/debug
 $ cmake -DCMAKE_BUILD_TYPE=Debug ../../
 $ ccmake .          # adjust CUDA SDK paths to your system!
 $ make -j
 $ ctest             # run tests to see if it went well
 $ make install
 $ export PYTHONPATH=`pwd`/src/python_bindings      # only if you want python bindings

Building a release version:

 $ tar xzvf cuv-version-source.tar.gz
 $ cd cuv-version-source
 $ mkdir -p build/release
 $ cd build/release
 $ cmake -DCMAKE_BUILD_TYPE=Release ../../
 $ ccmake .          # adjust CUDA SDK paths to your system!
 $ make -j
 $ ctest             # run tests to see if it went well
 $ export PYTHONPATH=`pwd`/src/python_bindings      # only if you want python bindings

Building the documentation

 $ cd build/debug    # change to the build directory
 $ make doc

Sample Code

We show two brief examples. For further inspiration, please take a look at the
test cases implemented in the src/tests directory.

Pushing and pulling of memory

C++ Code:

 #include <vector.hpp>
 using namespace cuv;

 vector<float,host_memory_space> h(256);  // reserves space in host memory
 vector<float,dev_memory_space>  d(256);  // reserves space in device memory

 fill(h,0);                          // terse form
 apply_0ary_functor(h,NF_FILL,0);    // more verbose

 convert(d,h);                       // push to device
 sequence(d);                        // fill device vector with a sequence

 convert(h,d);                       // pull to host
 for(int i=0;i<h.n();i++)
 {
   assert(d[i] == h[i]);
 }

Python Code:

 import pyublas
 import cuv_python as cp
 import numpy as np

 h = np.zeros((1,256)).astype("float32")                 # create numpy matrix
 d = cp.push(h)                                          # creates dev_matrix_rmf (row-major float) object

 h2 = np.zeros((1,256)).astype("float32").copy("F")      # create numpy matrix
 d2 = cp.push(h)                                         # creates dev_matrix_cmf (column-major float) object

 cp.fill(d,1)                                            # terse form
 cp.apply_nullary_functor(d,cp.nullary_functor.FILL,1)   # verbose form

 h = cp.pull(d)
 assert(h.sum() == 256)
 d.dealloc()                                             # explicitly deallocate memory (optional)

Simple Matrix operations

C++-Code

 #include <dense_matrix.hpp>
 #include <matrix_ops.hpp>
 using namespace cuv;

 dense_matrix<float,column_major,dev_memory_space> C(2048,2048),A(2048,2048),B(2048,2048);

 fill(C,0);         // initialize to some defined value, not strictly necessary here
 sequence(A);
 sequence(B);

 apply_binary_functor(A,B,BF_MULT);  // elementwise multiplication
 prod(C,A,B, 'n','t');               // matrix multiplication

Python Code

 import pyublas
 import cuv_python as cp
 import numpy as np
 C = cp.dev_matrix_cmf(2048,2048)   # cmf = column_major float
 A = cp.dev_matrix_cmf(2048,2048)
 B = cp.dev_matrix_cmf(2048,2048)
 cp.fill(C,0)                       # fill with some defined values, not really necessary here
 cp.sequence(A,0)
 cp.sequence(B,0)
 cp.apply_binary_functor(B,A,cp.binary_functor.MULT) # elementwise multiplication
 cp.prod(C,A,B,'n','t')                              # matrix multiplication
